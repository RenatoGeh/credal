% This is a minimal example of neural ADs in pasp.
%
% In this example, we will declare a neural annotated disjunction f(X, {a, b, c}) pointing to a
% neural network defined by function @net, and will learn its parameters so that the distribution
% of f(X, {a, b, c}) follows
%   ℙ(f([0, 0], {a, b, c})) = [2/8, 5/8, 1/8],
%   ℙ(f([0, 1], {a, b, c})) = [1/4, 2/4, 1/4],
%   ℙ(f([1, 0], {a, b, c})) = [1/5, 2/5, 2/5],
%   ℙ(f([1, 1], {a, b, c})) = [1/3, 1/3, 1/3].
%

#python
def train():
  # Input of the neural network during training.
  return [*[[0., 0.]]*8, *[[0., 1.]]*4, *[[1., 0.]]*5, *[[1., 1.]]*3]

def test():
  # Network inputs for evaluation.
  return [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]

def dataset():
  # The dataset defines the distribution:
  #   [2/8, 5/8, 1/8], [1/4, 2/4, 1/4], [1/5, 2/5, 2/5], [1/3, 1/3, 1/3]
  # for each of the 4 possible network inputs [0., 0.], [0., 1.], [1., 0.], [1., 1.].
  return [["label(a)"], ["label(a)"], ["label(b)"], ["label(b)"], ["label(b)"], ["label(b)"],
          ["label(b)"], ["label(c)"], ["label(a)"], ["label(b)"], ["label(b)"], ["label(c)"],
          ["label(a)"], ["label(b)"], ["label(b)"], ["label(c)"], ["label(c)"], ["label(a)"],
          ["label(b)"], ["label(c)"]]
#end.

% Just like in neural rules, predicate g(x) defines the data (train and test datasets) to be used
% in the network.
g(x) ~ test(@test), train(@train).

% The neural network is defined in function test_ad_mlp in hub/hubconf.py as a PyTorch Hub model.
?::f(X, {a, b, c}) as @test_ad_mlp on "hub" :- g(X).
label(Y) :- f(x, Y).

% We want to evaluate the outputs of the network f (here characterized by possible values a, b, c).
% The presence of parentheses surrounding queries is optional.
#query label(a).
#query label(b).
#query label(c).

% Parentheses are optional.
#semantics maxent.
#learn @dataset, lr = 0.1, niters = 2000, alg = "lagrange".
