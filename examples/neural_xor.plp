% A neural noisy XOR example.
%
% In this example, we will declare a neural predicate xor(X) that encodes some uncertainty on the
% measurement of the XOR of two bits X and Y, be it due to noisy data or some measurement error.
% In practical terms, the network should encode a probability distribution over the xor function;
% that is, the following probability table:
%
%    +---+---+-------+----------+
%    | X | Y | X ⊻ Y | ℙ(X ⊻ Y) |
%    +---+---+-------+----------+
%    | 0 | 0 |   0   |    .0    |
%    | 0 | 1 |   1   |    .7    |
%    | 1 | 0 |   1   |    .6    |
%    | 1 | 1 |   0   |    .0    |
%    +---+---+-------+----------+
%

#python
# Set manual seed for reproducibility.
torch.manual_seed(0)

def xor_net():
  # Define the neural network as a two layer network with sigmoids as activation functions.
  N = torch.nn.Sequential(
    torch.nn.Linear(2, 2),
    torch.nn.Sigmoid(),
    torch.nn.Linear(2, 1),
    # The sigmoid restricts the output to the [0, 1] interval.
    torch.nn.Sigmoid()
  )
  return N

def input_train():
  # Input of the neural network during training: a 40x2 matrix with 10 examples of each of the four
  # possible configurations of inputs.
  return [*[[0., 0.]]*10, *[[0., 1.]]*10, *[[1., 0.]]*10, *[[1., 1.]]*10]

def input_test():
  # Network inputs for evaluation: a 4x2 matrix with all input configurations.
  return [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]

def output_train():
  # The training dataset containing the observed atoms.
  return [*[["zero"]]*10,               # ℙ(0 ⊻ 0) = 0
          *[["one"]]*7, *[["zero"]]*3,  # ℙ(0 ⊻ 1) = 7/(7+3) = 0.7
          *[["one"]]*6, *[["zero"]]*4,  # ℙ(1 ⊻ 0) = 6/(6+4) = 0.6
          *[["zero"]]*10,               # ℙ(1 ⊻ 1) = 0
         ]
#end.

% The data predicate declares test and train datasets to be used by neural predicates and neural
% annotated disjunctions. You can name the data predicate anything you want as long as it is also a
% valid predicate name (starts with lower-case and only contains letters), but must have an arity
% of one.
data(input) ~ test(@input_test), train(@input_train).
% Neural rule xor(X) is defined by a function (here xor_net), with optimizer parameters given after
% the with keyword. Neural components must always come accompanied of a data predicate (in this
% case, data(X)).
?::xor(Input) as @xor_net with momentum = 0.1 :- data(Input).

% Here we are aliasing the output of xor(X). When xor(X) is true, then atom one holds; otherwise,
% zero holds.
one  :- xor(input).
zero :- not xor(input).

% We want to evaluate the probability of true (being true) for each of the input configurations in
% the Python function input_test.
#query(one).
% We define the maxent semantics to be used. The stable model semantics is set by default.
#semantics(maxent).
% We inject the return value of the output_train function to #learn and pass learning parameters.
#learn(@output_train, lr = 0.01, niters = 10000, alg = "lagrange").
