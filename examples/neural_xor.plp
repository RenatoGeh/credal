% A neural XOR example.
%
% In this example, we will declare a neural predicate xor(X) that should encode a probability
% distribution over the xor function; that is, the following probability table:
%
%    +---+---+-------+----------+
%    | X | Y | X ⊻ Y | ℙ(X ⊻ Y) |
%    +---+---+-------+----------+
%    | 0 | 0 |   0   |    .0    |
%    | 0 | 1 |   1   |    .3    |
%    | 1 | 0 |   1   |    .6    |
%    | 1 | 1 |   0   |    .0    |
%    +---+---+-------+----------+
%

#python
def xor_net():
  # Define the neural network as a two layer network with sigmoids as activation functions.
  N = torch.nn.Sequential(
    torch.nn.Linear(2, 2),
    torch.nn.Sigmoid(),
    torch.nn.Linear(2, 1),
    # The sigmoid restricts the output to the [0, 1] interval.
    torch.nn.Sigmoid()
  )
  return N

def input_train():
  # Input of the neural network during training: a 40x2 matrix with 10 examples of each of the four
  # possible configurations of inputs.
  return [*[[0., 0.]]*10, *[[0., 1.]]*10, *[[1., 0.]]*10, *[[1., 1.]]*10]

def input_test():
  # Network inputs for evaluation: a 4x2 matrix with all input configurations.
  return [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]

def output_train():
  # The training dataset containing the observed atoms.
  return [*[["false"]]*10,               # ℙ(0 ⊻ 0) = 0
          *[["true"]]*3, *[["false"]]*7, # ℙ(0 ⊻ 1) = 3/(3+7) = 0.3
          *[["true"]]*6, *[["false"]]*4, # ℙ(1 ⊻ 0) = 6/(6+4) = 0.6
          *[["false"]]*10,               # ℙ(1 ⊻ 1) = 0
         ]
#end.

% The data predicate declares test and train datasets to be used by neural predicates and neural
% annotated disjunctions. You can name the data predicate anything you want as long as it is also a
% valid predicate name (starts with lower-case and only contains letters), but must have an arity
% of one.
data(x) ~ test(@input_test), train(@input_train).
% Neural rule xor(X) is defined by a function (here xor_net), with optimizer parameters given after
% the with keyword. Neural components must always come accompanied of a data predicate (in this
% case, data(X)).
?::xor(X) as @xor_net :- data(X).

% Here we are aliasing the output of xor(X). When xor(X) is true, atom true is true; otherwise,
% false is true.
true  :- xor(x).
false :- not xor(x).

% We want to evaluate the probability of true (being true) for each of the input configurations in
% the Python function input_test.
#query(true).
% We define the maxent semantics to be used. The stable model semantics is set by default.
#semantics(maxent).
% We inject the return value of the output_train function to #learn and pass learning parameters.
#learn(@output_train, lr = 0.1, niters = 10000, alg = "lagrange").
