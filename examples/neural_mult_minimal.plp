% This is the simplest scenario possible for neural rules with multiple outcomes in pasp.
%
% A neural predicate has k outcomes when its neural network outputs k probability distributions.
%
% In this example, we will declare a neural predicate f(X, Y) pointing to a neural network defined
% by function @net with 3 outcomes, and will learn its parameters so that the distributions of
% f(X, Y) follow
%   ℙ(f(0, a)) = 0.25, ℙ(f(0, b)) = 0.70,  ℙ(f(0, c)) = 0.10,
%   ℙ(f(1, a)) = 0.90, ℙ(f(1, b)) = 0.50,  ℙ(f(1, c)) = 0.75.

#python
def net():
  # Define the neural network as a single neuron with a sigmoid as activation function.
  N = torch.nn.Sequential(
    torch.nn.Linear(1, 3, bias = True),
    # Make sure the output is always in [0, 1].
    torch.nn.Sigmoid()
  )
  return N

def target():
  N = torch.nn.Sequential(
    torch.nn.Linear(1, 3, bias = True),
    # Make sure the output is always in [0, 1].
    torch.nn.Sigmoid()
  )
  S = {'0.weight': torch.tensor([[3.2948], [-0.8472], [3.2942]]),
   '0.bias': torch.tensor([-1.0984, 0.8472, -2.1958])}
  N.load_state_dict(S)
  return N

def train():
  # Input of the neural network during training.
  return [*[[0.]]*20, *[[1.]]*20]

def test():
  # Network inputs for evaluation.
  return [[0.], [1.]]

def dataset():
  # Target probabilities:
  #   ℙ(f(x, a) | Z = 0) = 0.25 | ℙ(f(x, b) | Z = 0) = 0.70 | ℙ(f(x, c) | Z = 0) = 0.10
  #   ℙ(f(x, a) | Z = 1) = 0.90 | ℙ(f(x, b) | Z = 1) = 0.50 | ℙ(f(x, c) | Z = 1) = 0.75
  return [[
    "f(x, a)" if i < 5 else "~f(x, a)",
    "f(x, b)" if i < 14 else "~f(x, b)",
    "f(x, c)" if i < 2 else "~f(x, c)",
  ] for i in range(20)] + [[
    "f(x, a)" if i < 18 else "~f(x, a)",
    "f(x, b)" if i < 10 else "~f(x, b)",
    "f(x, c)" if i < 15 else "~f(x, c)",
  ] for i in range(20)]
#end.

% Predicate g(x) is a special data predicate for declaring test and train datasets.
g(x) ~ test(@test), train(@train).

% Neural rule f(X) is defined by the function net, with optimizer parameters given after the with
% keyword. Neural components must always come accompanied of a data predicate (in this case, g(X)).
?::f(X; {a, b, c}) as @target with lr = 0.1, momentum = 0.1 :- g(X).

% For this case, f(X) is grounded to a single neural fact f(x). Let's evaluate its probability.
#query(f(x; a)).
#query(f(x; b)).
#query(f(x; c)).

#semantics(maxent).
#learn(@dataset, lr = 0.1, niters = 500, alg = "lagrange").
